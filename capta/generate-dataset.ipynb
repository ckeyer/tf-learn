{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 17)\n",
      "tf.Tensor(\n",
      "[[1.         1.         0.98039216 0.9882353  1.         1.\n",
      "  0.98039216 1.         1.         1.         0.99607843 1.\n",
      "  1.         1.         0.99215686 0.9843137  1.        ]\n",
      " [0.9490196  1.         1.         0.972549   0.96862745 0.9764706\n",
      "  0.98039216 1.         1.         0.99215686 0.99215686 0.9882353\n",
      "  0.9882353  0.99215686 1.         1.         1.        ]\n",
      " [1.         1.         0.9843137  1.         1.         1.\n",
      "  0.98039216 1.         1.         1.         1.         1.\n",
      "  0.99215686 0.9882353  0.99607843 1.         1.        ]\n",
      " [1.         0.95686275 0.83137256 0.7921569  0.8980392  0.99607843\n",
      "  1.         0.98039216 0.98039216 0.99607843 1.         1.\n",
      "  1.         1.         0.99607843 0.9843137  1.        ]\n",
      " [0.9882353  1.         0.8392157  0.6156863  0.65882355 0.8862745\n",
      "  1.         1.         0.9764706  0.9882353  0.9882353  0.98039216\n",
      "  0.9882353  1.         1.         0.99607843 1.        ]\n",
      " [1.         1.         0.972549   0.827451   0.7137255  0.69411767\n",
      "  0.8117647  0.96862745 1.         1.         1.         0.9882353\n",
      "  0.98039216 1.         1.         1.         1.        ]\n",
      " [0.9882353  0.98039216 0.99215686 1.         0.90588236 0.69803923\n",
      "  0.6392157  0.75686276 0.9098039  0.9843137  1.         1.\n",
      "  0.99215686 0.99607843 1.         0.99215686 1.        ]\n",
      " [0.9843137  1.         1.         0.9764706  1.         0.9882353\n",
      "  0.8039216  0.6313726  0.6666667  0.81960785 0.96862745 1.\n",
      "  1.         1.         1.         1.         1.        ]\n",
      " [1.         0.98039216 1.         0.9843137  1.         1.\n",
      "  1.         0.83137256 0.63529414 0.4862745  0.6862745  0.8745098\n",
      "  0.9607843  0.9764706  1.         0.9882353  1.        ]\n",
      " [0.972549   1.         1.         1.         0.9882353  0.81960785\n",
      "  0.7254902  0.6509804  0.654902   0.62352943 0.57254905 0.47058824\n",
      "  0.6509804  0.84705883 0.98039216 0.9764706  1.        ]\n",
      " [1.         0.99215686 0.99215686 1.         0.77254903 0.6313726\n",
      "  0.74509805 0.8627451  0.93333334 0.9764706  0.9254902  0.6901961\n",
      "  0.5803922  0.5294118  0.73333335 0.93333334 1.        ]\n",
      " [1.         0.9529412  0.9882353  0.9098039  0.4745098  0.65882355\n",
      "  1.         0.98039216 0.8980392  0.87058824 0.9607843  0.972549\n",
      "  0.7882353  0.4627451  0.52156866 0.65882355 0.7921569 ]\n",
      " [0.99215686 0.99607843 1.         0.80784315 0.38039216 0.62352943\n",
      "  0.8784314  0.60784316 0.42745098 0.48235294 0.61960787 0.8666667\n",
      "  0.94509804 0.58431375 0.54901963 0.72156864 0.60784316]\n",
      " [0.9882353  1.         0.9882353  0.8862745  0.5568628  0.3764706\n",
      "  0.42745098 0.4627451  0.5137255  0.47843137 0.44313726 0.7764706\n",
      "  1.         0.5803922  0.5803922  1.         0.9019608 ]\n",
      " [1.         0.9882353  0.9843137  1.         0.8039216  0.4392157\n",
      "  0.5019608  0.7647059  0.94509804 0.5921569  0.4627451  0.8352941\n",
      "  0.972549   0.54901963 0.654902   0.99215686 0.99607843]\n",
      " [1.         0.972549   1.         0.99215686 0.94509804 0.85882354\n",
      "  0.99215686 0.96862745 0.8666667  0.5647059  0.7176471  1.\n",
      "  0.7607843  0.5058824  0.9019608  1.         0.99607843]\n",
      " [1.         0.99215686 1.         0.99607843 0.95686275 1.\n",
      "  1.         0.90588236 0.6117647  0.7372549  1.         0.98039216\n",
      "  0.5411765  0.7137255  1.         0.9647059  1.        ]\n",
      " [0.9882353  0.99607843 0.99215686 0.99607843 1.         1.\n",
      "  0.84705883 0.6039216  0.77254903 0.9764706  0.8980392  0.6901961\n",
      "  0.6509804  0.9411765  1.         0.99215686 1.        ]\n",
      " [0.9764706  1.         1.         0.9882353  1.         0.8745098\n",
      "  0.74509805 0.7058824  0.99607843 0.9607843  0.7058824  0.69411767\n",
      "  0.8901961  1.         0.9411765  1.         0.99215686]\n",
      " [0.99607843 0.99215686 0.99607843 0.99215686 0.9490196  0.67058825\n",
      "  0.7019608  1.         0.9490196  0.5921569  0.44705883 0.7137255\n",
      "  0.7882353  0.7411765  0.7058824  0.81960785 0.9882353 ]], shape=(20, 17), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def image_load(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image_tensor=tf.image.decode_jpeg(image,channels=3)\n",
    "    image_tensor=tf.image.rgb_to_grayscale(image_tensor)\n",
    "    image_tensor=tf.cast(image_tensor,tf.float32)\n",
    "    image_tensor=tf.squeeze(image_tensor)\n",
    "    image_tensor=image_tensor/255.\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "img = image_load('/tmp/capba/2-3g2vx6rz-0-053-1-1-1.jpg')\n",
    "print(img.shape)\n",
    "print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "30.0\n",
      "2\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alllabels = \"23456789abcdefghjkmnpqrstuvwxyz\"\n",
    "\n",
    "def label_to_index(label):\n",
    "    for i in range(len(alllabels)):\n",
    "        if alllabels[i] == label:\n",
    "            return float(i)\n",
    "\n",
    "print(label_to_index('3'))\n",
    "print(label_to_index('z'))\n",
    "\n",
    "def index_to_label(idx):\n",
    "    return alllabels[idx]\n",
    "\n",
    "print(index_to_label(0))\n",
    "print(index_to_label(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49280\n",
      "(49280, 20, 17) (49280,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "image_dataset_perelement_numpy=[]\n",
    "image_dataset_labels=[]\n",
    "for root, dirs, files in os.walk(\"/tmp/capba\", topdown=False):\n",
    "    image_count=len(files)\n",
    "    for name in files:\n",
    "        filepath = os.path.join(root, name)\n",
    "        try:\n",
    "            img = image_load(filepath)\n",
    "            image_dataset_perelement_numpy.append(np.array(img))\n",
    "            image_dataset_labels.append(label_to_index(name[0]))\n",
    "        finally:\n",
    "            None\n",
    "            # print('failed', filepath)\n",
    "        \n",
    "\n",
    "image_dataset=np.array(image_dataset_perelement_numpy)\n",
    "all_labels=np.array(image_dataset_labels)\n",
    "\n",
    "print(image_count)\n",
    "print(image_dataset.shape, all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36468, 20, 17) (36468,)\n",
      "(6406, 20, 17) (6406,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_count=int(image_count*0.13)\n",
    "train_count=image_count-test_count\n",
    "\n",
    "train_image=image_dataset[test_count:train_count]\n",
    "train_label=all_labels[test_count:train_count]\n",
    "test_image=image_dataset[:test_count]\n",
    "test_label=all_labels[:test_count]\n",
    "\n",
    "print(train_image.shape, train_label.shape)\n",
    "print(test_image.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(20, 17)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(31, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1140/1140 [==============================] - 2s 1ms/step - loss: 3.2125 - accuracy: 0.0868\n",
      "Epoch 2/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 2.7436 - accuracy: 0.1813\n",
      "Epoch 3/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 2.3005 - accuracy: 0.2994\n",
      "Epoch 4/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 1.8730 - accuracy: 0.4282\n",
      "Epoch 5/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 1.5491 - accuracy: 0.5203\n",
      "Epoch 6/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 1.3080 - accuracy: 0.5934\n",
      "Epoch 7/25\n",
      "1140/1140 [==============================] - 2s 1ms/step - loss: 1.1489 - accuracy: 0.6445\n",
      "Epoch 8/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 1.0312 - accuracy: 0.6791\n",
      "Epoch 9/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.9349 - accuracy: 0.7097\n",
      "Epoch 10/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.8627 - accuracy: 0.7316\n",
      "Epoch 11/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.7974 - accuracy: 0.7528\n",
      "Epoch 12/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.7467 - accuracy: 0.7699\n",
      "Epoch 13/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.7057 - accuracy: 0.7829\n",
      "Epoch 14/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.6559 - accuracy: 0.7960\n",
      "Epoch 15/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.6346 - accuracy: 0.8070\n",
      "Epoch 16/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.5970 - accuracy: 0.8156\n",
      "Epoch 17/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.5692 - accuracy: 0.8241\n",
      "Epoch 18/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.5585 - accuracy: 0.8273\n",
      "Epoch 19/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.5293 - accuracy: 0.8370\n",
      "Epoch 20/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.5052 - accuracy: 0.8440\n",
      "Epoch 21/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.4854 - accuracy: 0.8519\n",
      "Epoch 22/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.4756 - accuracy: 0.8553\n",
      "Epoch 23/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.4604 - accuracy: 0.8613\n",
      "Epoch 24/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.4450 - accuracy: 0.8635\n",
      "Epoch 25/25\n",
      "1140/1140 [==============================] - 1s 1ms/step - loss: 0.4244 - accuracy: 0.8722\n",
      "201/201 - 0s - loss: 0.3741 - accuracy: 0.8935 - 222ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3741467297077179, 0.8935372829437256]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_image, train_label, epochs=25)\n",
    "\n",
    "model.evaluate(test_image,  test_label, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('capta-002.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def topResult(result):\n",
    "    topIdx = 0\n",
    "    for i in range(len(result)):\n",
    "        if result[i] > result[topIdx]:\n",
    "            topIdx=i\n",
    "    return index_to_label(topIdx)\n",
    "    # print(topIdx, index_to_label(topIdx), result[topIdx])\n",
    " \n",
    "procmodel = tf.keras.models.load_model(\"capta-002.h5\")\n",
    "\n",
    "imgdata=image_load('/tmp/capab/d-5qgxd1bh-2-680,jpg')\n",
    "\n",
    "result=procmodel.predict(np.array([imgdata,]))\n",
    "\n",
    "# for v in np.nditer(np.sort(result), order='C'):\n",
    "#     print(v)\n",
    "# print(len(np.nditer(result, order='C')))\n",
    "result = result[0].tolist()\n",
    "topResult(result)\n",
    "# for i in range(len(result)):\n",
    "#     # print(i)\n",
    "#     print(i, index_to_label(i), result[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"/tmp/capac\", topdown=False):\n",
    "    for name in files:\n",
    "        filepath = os.path.join(root, name)\n",
    "        imgdata = image_load(filepath)\n",
    "        result=procmodel.predict(np.array([imgdata,]))\n",
    "        result = result[0].tolist()\n",
    "        newname = '%s-%s'%(topResult(result), name)\n",
    "        newpath = os.path.join(root, newname)\n",
    "        # print(filepath, newpath)\n",
    "        os.rename(filepath, newpath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fnpx\n"
     ]
    }
   ],
   "source": [
    "def predict(imgs):\n",
    "    imgdatas = []\n",
    "    for img in imgs:\n",
    "        imgdatas.append(image_load(img))\n",
    "    result = procmodel.predict(np.array(imgdatas))\n",
    "    resultStr = ''\n",
    "    for i in range(len(imgs)):\n",
    "        resultStr += topResult(result[i].tolist())\n",
    "    return resultStr\n",
    "\n",
    "res = predict([\n",
    "    '/tmp/capac/n-sy57u_up-0-000,jpg',\n",
    "    '/tmp/capac/p-sy57u_up-1-000,jpg',\n",
    "    '/tmp/capac/x-sy57u_up-2-000,jpg',\n",
    "    '/tmp/capac/f-sy57u_up-3-000,jpg',])\n",
    "print(res)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "541649e5934e024eedcf6b32ceb5760adff4f31b8d11651c2bbc09e50763baa6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf-learn-ELLEaoWI': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
